{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93407648",
   "metadata": {},
   "source": [
    "# LangChain & LangGraph Practice: Building Agentic AI Workflows\n",
    "\n",
    "Welcome to this hands-on practice notebook for building Agentic AI workflows using LangChain and LangGraph!\n",
    "\n",
    "## What You'll Learn:\n",
    "- 🔧 Set up LangChain and LangGraph environments\n",
    "- 🤖 Create simple and complex AI agents\n",
    "- 🔗 Build graph-based workflows with multiple agents\n",
    "- 🛠️ Integrate external tools and APIs\n",
    "- 💾 Implement memory and state management\n",
    "- 🔀 Create conditional workflow paths\n",
    "- 🧪 Test and debug agent workflows\n",
    "\n",
    "## Prerequisites:\n",
    "- Basic Python knowledge\n",
    "- OpenAI API key (or other LLM provider)\n",
    "- Understanding of AI/ML concepts (helpful but not required)\n",
    "\n",
    "Let's start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e8770",
   "metadata": {},
   "source": [
    "## 1. Setup and Install Dependencies\n",
    "\n",
    "First, let's install all the required packages for our LangChain and LangGraph practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2023c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ langchain is already installed\n",
      "✅ langchain-community is already installed\n",
      "✅ langchain-openai is already installed\n",
      "✅ langgraph is already installed\n",
      "📦 Installing python-dotenv...\n",
      "Requirement already satisfied: python-dotenv in /Users/davidkoh/devel/llm/agenticai/.venv/lib/python3.13/site-packages (1.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ matplotlib is already installed\n",
      "✅ pandas is already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Run this cell if packages are not already installed\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",\n",
    "    \"langchain-community\", \n",
    "    \"langchain-openai\",\n",
    "    \"langgraph\",\n",
    "    \"python-dotenv\",\n",
    "    \"matplotlib\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"✅ {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"📦 Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062f253",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all the necessary libraries for building our agentic workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d79a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 All libraries imported successfully!\n",
      "🚀 Ready to build agentic workflows!\n",
      "ℹ️ Note: ToolExecutor import updated for compatibility\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "from typing import TypedDict, Annotated, Literal, List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "# Note: ToolExecutor has been moved/deprecated in newer versions\n",
    "# We'll create tools directly when needed\n",
    "\n",
    "# Utility imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"📚 All libraries imported successfully!\")\n",
    "print(\"🚀 Ready to build agentic workflows!\")\n",
    "print(\"ℹ️ Note: ToolExecutor import updated for compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0fb10",
   "metadata": {},
   "source": [
    "## 3. Configure API Keys and Environment\n",
    "\n",
    "⚠️ **Important**: You'll need to set up your API keys to run the examples.\n",
    "\n",
    "### Required: LLM Provider API Key\n",
    "You need an API key from an LLM provider like OpenAI, Anthropic, etc.\n",
    "\n",
    "### Option 1: Create a `.env` file\n",
    "Create a `.env` file in your project directory with:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "```\n",
    "\n",
    "### Option 2: Set directly in the notebook (less secure)\n",
    "Uncomment and modify the lines below.\n",
    "\n",
    "### Optional: LangSmith (Monitoring & Debugging)\n",
    "LangSmith is LangChain's optional monitoring service. It's separate from LangChain itself:\n",
    "```\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_API_KEY=your_langsmith_api_key_here\n",
    "```\n",
    "\n",
    "**Note**: LangChain framework itself is free and open-source - no API key required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17407f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ OpenAI API key not found\n",
      "📝 Please set OPENAI_API_KEY in your .env file or directly in the cell above\n",
      "ℹ️ LangSmith tracing not configured (optional for learning)\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Option 2: Set API keys directly (uncomment and modify)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your_langsmith_api_key_here\"\n",
    "\n",
    "# Check if LLM API key is set (REQUIRED)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"✅ OpenAI API key is configured\")\n",
    "    print(f\"🔑 Key starts with: {api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"❌ OpenAI API key not found\")\n",
    "    print(\"📝 Please set OPENAI_API_KEY in your .env file or directly in the cell above\")\n",
    "    print(\"💡 You can get an API key from: https://platform.openai.com/api-keys\")\n",
    "\n",
    "# Optional: Check LangSmith setup (for monitoring/debugging)\n",
    "if os.getenv(\"LANGCHAIN_API_KEY\"):\n",
    "    print(\"✅ LangSmith tracing is configured (optional)\")\n",
    "    print(\"🔍 This enables monitoring and debugging of your chains\")\n",
    "else:\n",
    "    print(\"ℹ️ LangSmith tracing not configured (optional)\")\n",
    "    print(\"💡 LangSmith is a separate monitoring service - not required for learning!\")\n",
    "\n",
    "print(\"\\n🎯 Summary:\")\n",
    "print(\"• LangChain = Free, open-source framework (no API key)\")\n",
    "print(\"• OpenAI = LLM provider (API key required)\")  \n",
    "print(\"• LangSmith = Optional monitoring service (separate API key)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5cd6a",
   "metadata": {},
   "source": [
    "## 4. Create a Simple LangChain Agent\n",
    "\n",
    "Let's start with a basic LangChain agent that can use tools to perform tasks.\n",
    "\n",
    "### Exercise 4.1: Simple Calculator Agent\n",
    "We'll create an agent that can perform mathematical calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple calculator tool\n",
    "def calculator_tool(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs mathematical calculations.\n",
    "    Args:\n",
    "        expression: A mathematical expression as a string (e.g., \"2 + 3 * 4\")\n",
    "    Returns:\n",
    "        The result of the calculation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Be careful with eval in production! This is for learning purposes.\n",
    "        result = eval(expression)\n",
    "        return f\"The result of {expression} is: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating {expression}: {str(e)}\"\n",
    "\n",
    "# Create the tool\n",
    "calculator = Tool(\n",
    "    name=\"calculator\",\n",
    "    description=\"Useful for performing mathematical calculations. Input should be a mathematical expression.\",\n",
    "    func=calculator_tool\n",
    ")\n",
    "\n",
    "# Test the tool\n",
    "print(\"🧮 Testing calculator tool:\")\n",
    "print(calculator.run(\"15 + 25\"))\n",
    "print(calculator.run(\"(10 * 5) / 2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba552e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "# Create tools list\n",
    "tools = [calculator]\n",
    "\n",
    "# Get the prompt from LangChain hub\n",
    "try:\n",
    "    prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "    print(\"✅ Prompt loaded from LangChain hub\")\n",
    "except:\n",
    "    # Fallback prompt if hub is not available\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that can use tools to help answer questions.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "    print(\"✅ Using fallback prompt\")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"🤖 Calculator agent created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with different mathematical queries\n",
    "test_queries = [\n",
    "    \"What is 15 multiplied by 23?\",\n",
    "    \"Calculate the area of a circle with radius 5 (use π = 3.14159)\",\n",
    "    \"If I invest $1000 at 5% annual interest for 3 years, how much will I have? Use compound interest formula.\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing the calculator agent:\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"📝 Test {i}: {query}\")\n",
    "    try:\n",
    "        result = agent_executor.invoke({\"input\": query})\n",
    "        print(f\"✅ Result: {result['output']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8042258",
   "metadata": {},
   "source": [
    "## 5. Build Basic LangGraph Workflow\n",
    "\n",
    "Now let's create a simple graph-based workflow using LangGraph. This workflow will analyze a problem and provide recommendations.\n",
    "\n",
    "### Exercise 5.1: Problem Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our workflow\n",
    "class AnalysisState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    problem: str\n",
    "    analysis: str\n",
    "    recommendations: str\n",
    "    current_step: str\n",
    "\n",
    "# Define workflow nodes\n",
    "def analyze_problem(state: AnalysisState) -> AnalysisState:\n",
    "    \"\"\"Analyze the user's problem\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert analyst. Analyze the given problem thoroughly, identifying key issues, root causes, and important factors.\"),\n",
    "        (\"human\", \"Problem: {problem}\\n\\nProvide a detailed analysis:\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    analysis = chain.invoke({\"problem\": state[\"problem\"]})\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"analysis\": analysis,\n",
    "        \"current_step\": \"analysis_complete\"\n",
    "    }\n",
    "\n",
    "def generate_recommendations(state: AnalysisState) -> AnalysisState:\n",
    "    \"\"\"Generate recommendations based on the analysis\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Based on the analysis provided, generate 3-5 practical, actionable recommendations with clear steps.\"),\n",
    "        (\"human\", \"Problem: {problem}\\n\\nAnalysis: {analysis}\\n\\nProvide specific recommendations:\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    recommendations = chain.invoke({\n",
    "        \"problem\": state[\"problem\"],\n",
    "        \"analysis\": state[\"analysis\"]\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"recommendations\": recommendations,\n",
    "        \"current_step\": \"recommendations_complete\"\n",
    "    }\n",
    "\n",
    "print(\"✅ Workflow nodes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1dd3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workflow graph\n",
    "workflow = StateGraph(AnalysisState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"analyze\", analyze_problem)\n",
    "workflow.add_node(\"recommend\", generate_recommendations)\n",
    "\n",
    "# Define the flow\n",
    "workflow.set_entry_point(\"analyze\")\n",
    "workflow.add_edge(\"analyze\", \"recommend\")\n",
    "workflow.add_edge(\"recommend\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"🔄 LangGraph workflow created!\")\n",
    "\n",
    "# Test the workflow\n",
    "test_problems = [\n",
    "    \"I want to learn machine learning but don't know where to start and feel overwhelmed by all the options.\",\n",
    "    \"My team is struggling with communication in remote work setup.\",\n",
    "    \"I need to decide between Python and JavaScript for my next web development project.\"\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 Testing the analysis workflow:\\n\")\n",
    "\n",
    "for i, problem in enumerate(test_problems, 1):\n",
    "    print(f\"📝 Problem {i}: {problem}\")\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"messages\": [],\n",
    "        \"problem\": problem,\n",
    "        \"analysis\": \"\",\n",
    "        \"recommendations\": \"\",\n",
    "        \"current_step\": \"start\"\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n📊 Analysis:\\n{result['analysis']}\")\n",
    "    print(f\"\\n💡 Recommendations:\\n{result['recommendations']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca93fa",
   "metadata": {},
   "source": [
    "## 6. Create Multi-Agent Workflow\n",
    "\n",
    "Let's build a more complex workflow with multiple specialized agents that collaborate on tasks.\n",
    "\n",
    "### Exercise 6.1: Content Creation Workflow\n",
    "We'll create a workflow with different agents for different types of content creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for multi-agent workflow\n",
    "class MultiAgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    task_type: str\n",
    "    user_input: str\n",
    "    content: str\n",
    "    review_feedback: str\n",
    "    final_output: str\n",
    "    iteration_count: int\n",
    "\n",
    "# Classifier agent\n",
    "def classify_task(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Classify the type of content creation task\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Classify the user's request into one of these categories:\n",
    "        - 'technical': Code, documentation, technical explanations\n",
    "        - 'creative': Stories, poems, creative writing\n",
    "        - 'business': Reports, proposals, business content\n",
    "        - 'educational': Tutorials, explanations, learning materials\n",
    "        \n",
    "        Respond with just the category name.\"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    task_type = chain.invoke({\"input\": state[\"user_input\"]}).strip().lower()\n",
    "    \n",
    "    return {**state, \"task_type\": task_type, \"iteration_count\": 0}\n",
    "\n",
    "# Specialized agents\n",
    "def technical_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Create technical content\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a technical expert. Create accurate, detailed technical content with examples and clear explanations.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    content = chain.invoke({\"input\": state[\"user_input\"]})\n",
    "    \n",
    "    return {**state, \"content\": content}\n",
    "\n",
    "def creative_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Create creative content\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a creative writer. Create engaging, imaginative content with vivid descriptions and compelling narratives.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    content = chain.invoke({\"input\": state[\"user_input\"]})\n",
    "    \n",
    "    return {**state, \"content\": content}\n",
    "\n",
    "def business_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Create business content\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a business professional. Create clear, professional business content with structured formatting and actionable insights.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    content = chain.invoke({\"input\": state[\"user_input\"]})\n",
    "    \n",
    "    return {**state, \"content\": content}\n",
    "\n",
    "def educational_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Create educational content\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.4)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an educator. Create clear, step-by-step educational content with examples and practical exercises.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    content = chain.invoke({\"input\": state[\"user_input\"]})\n",
    "    \n",
    "    return {**state, \"content\": content}\n",
    "\n",
    "print(\"✅ Multi-agent system defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09371f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing logic\n",
    "def route_to_agent(state: MultiAgentState) -> Literal[\"technical\", \"creative\", \"business\", \"educational\"]:\n",
    "    \"\"\"Route to the appropriate agent based on task type\"\"\"\n",
    "    task_type = state[\"task_type\"]\n",
    "    return task_type if task_type in [\"technical\", \"creative\", \"business\", \"educational\"] else \"educational\"\n",
    "\n",
    "# Review agent\n",
    "def review_content(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Review the generated content\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Review the content and provide feedback. Rate it 1-10 and suggest improvements.\n",
    "        If the score is 8 or above, respond with 'APPROVED: [score]/10 - [brief positive feedback]'\n",
    "        If below 8, provide specific improvement suggestions.\"\"\"),\n",
    "        (\"human\", \"Content to review: {content}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    feedback = chain.invoke({\"content\": state[\"content\"]})\n",
    "    \n",
    "    return {\n",
    "        **state, \n",
    "        \"review_feedback\": feedback,\n",
    "        \"iteration_count\": state[\"iteration_count\"] + 1\n",
    "    }\n",
    "\n",
    "def should_finalize(state: MultiAgentState) -> Literal[\"finalize\", \"revise\"]:\n",
    "    \"\"\"Decide whether to finalize or revise\"\"\"\n",
    "    feedback = state[\"review_feedback\"].lower()\n",
    "    max_iterations = 2\n",
    "    \n",
    "    if \"approved\" in feedback or state[\"iteration_count\"] >= max_iterations:\n",
    "        return \"finalize\"\n",
    "    else:\n",
    "        return \"revise\"\n",
    "\n",
    "def finalize_output(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Finalize the output\"\"\"\n",
    "    return {**state, \"final_output\": state[\"content\"]}\n",
    "\n",
    "def revise_content(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Revise content based on feedback\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Revise the content based on the feedback provided. Improve quality and address concerns.\"),\n",
    "        (\"human\", \"Original request: {user_input}\\n\\nCurrent content: {content}\\n\\nFeedback: {feedback}\\n\\nProvide improved version:\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    revised_content = chain.invoke({\n",
    "        \"user_input\": state[\"user_input\"],\n",
    "        \"content\": state[\"content\"],\n",
    "        \"feedback\": state[\"review_feedback\"]\n",
    "    })\n",
    "    \n",
    "    return {**state, \"content\": revised_content}\n",
    "\n",
    "print(\"✅ Routing and review logic defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multi-agent workflow\n",
    "multi_workflow = StateGraph(MultiAgentState)\n",
    "\n",
    "# Add all nodes\n",
    "multi_workflow.add_node(\"classify\", classify_task)\n",
    "multi_workflow.add_node(\"technical\", technical_agent)\n",
    "multi_workflow.add_node(\"creative\", creative_agent)\n",
    "multi_workflow.add_node(\"business\", business_agent)\n",
    "multi_workflow.add_node(\"educational\", educational_agent)\n",
    "multi_workflow.add_node(\"review\", review_content)\n",
    "multi_workflow.add_node(\"revise\", revise_content)\n",
    "multi_workflow.add_node(\"finalize\", finalize_output)\n",
    "\n",
    "# Define the flow\n",
    "multi_workflow.set_entry_point(\"classify\")\n",
    "\n",
    "# Route to appropriate agent\n",
    "multi_workflow.add_conditional_edges(\n",
    "    \"classify\",\n",
    "    route_to_agent,\n",
    "    {\n",
    "        \"technical\": \"technical\",\n",
    "        \"creative\": \"creative\", \n",
    "        \"business\": \"business\",\n",
    "        \"educational\": \"educational\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# All agents go to review\n",
    "multi_workflow.add_edge(\"technical\", \"review\")\n",
    "multi_workflow.add_edge(\"creative\", \"review\")\n",
    "multi_workflow.add_edge(\"business\", \"review\")\n",
    "multi_workflow.add_edge(\"educational\", \"review\")\n",
    "\n",
    "# Conditional routing after review\n",
    "multi_workflow.add_conditional_edges(\n",
    "    \"review\",\n",
    "    should_finalize,\n",
    "    {\n",
    "        \"finalize\": \"finalize\",\n",
    "        \"revise\": \"revise\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# After revision, go back to review\n",
    "multi_workflow.add_edge(\"revise\", \"review\")\n",
    "multi_workflow.add_edge(\"finalize\", END)\n",
    "\n",
    "# Compile the multi-agent workflow\n",
    "multi_app = multi_workflow.compile()\n",
    "\n",
    "print(\"🤖 Multi-agent workflow created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72028fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the multi-agent workflow\n",
    "test_requests = [\n",
    "    \"Write a Python function to implement bubble sort with comments\",\n",
    "    \"Create a short story about a time traveler who gets stuck in 1920s Paris\",\n",
    "    \"Draft a business proposal for a new employee wellness program\",\n",
    "    \"Explain how photosynthesis works with simple examples for high school students\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing multi-agent workflow:\\n\")\n",
    "\n",
    "for i, request in enumerate(test_requests, 1):\n",
    "    print(f\"📝 Request {i}: {request}\")\n",
    "    \n",
    "    result = multi_app.invoke({\n",
    "        \"messages\": [],\n",
    "        \"task_type\": \"\",\n",
    "        \"user_input\": request,\n",
    "        \"content\": \"\",\n",
    "        \"review_feedback\": \"\",\n",
    "        \"final_output\": \"\",\n",
    "        \"iteration_count\": 0\n",
    "    })\n",
    "    \n",
    "    print(f\"🏷️ Task Type: {result['task_type']}\")\n",
    "    print(f\"🔄 Iterations: {result['iteration_count']}\")\n",
    "    print(f\"📊 Final Review: {result['review_feedback'][:100]}...\")\n",
    "    print(f\"✅ Final Output:\\n{result['final_output'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21e8b3",
   "metadata": {},
   "source": [
    "## 7. Implement Tool Integration\n",
    "\n",
    "Let's add external tools and APIs to our agents for enhanced capabilities.\n",
    "\n",
    "## 8. Add Memory and State Management\n",
    "\n",
    "Implement conversation memory and state persistence.\n",
    "\n",
    "## 9. Create Conditional Workflow Paths\n",
    "\n",
    "Build dynamic workflows with conditional branching.\n",
    "\n",
    "## 10. Test and Debug Agent Workflows\n",
    "\n",
    "Create comprehensive test cases and debugging strategies.\n",
    "\n",
    "### 🎯 Practice Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Custom Tool Creation**: Create a tool that fetches weather data or current time\n",
    "2. **Memory Enhancement**: Add long-term memory to store user preferences\n",
    "3. **Complex Routing**: Create a workflow that routes based on multiple conditions\n",
    "4. **Error Handling**: Add robust error handling and recovery mechanisms\n",
    "5. **Performance Optimization**: Implement caching and optimize LLM calls\n",
    "\n",
    "### 📚 Next Steps\n",
    "\n",
    "- Explore LangSmith for monitoring and debugging\n",
    "- Try different LLM providers (Anthropic, Google, etc.)\n",
    "- Build domain-specific agents for your use case\n",
    "- Integrate with external APIs and databases\n",
    "- Deploy your agents to production\n",
    "\n",
    "### 🔗 Useful Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangSmith for Monitoring](https://smith.langchain.com/)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)\n",
    "\n",
    "Happy building! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4cf6c7",
   "metadata": {},
   "source": [
    "## 🚀 Quick Demo: Understanding LangChain Basics\n",
    "\n",
    "Let's start with a simple demonstration to understand how LangChain works, even without an API key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2664255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Understanding LangChain Building Blocks\n",
      "============================================================\n",
      "✅ Step 1: Prompt Template Created\n",
      "📝 This template has two variables: {subject} and {topic}\n",
      "\n",
      "📋 See how the template gets filled with actual values:\n",
      "\n",
      "🎯 Subject: programming, Topic: variables\n",
      "   System: You are a helpful assistant that explains programming concepts clearly.\n",
      "   Human:  Explain variables in simple terms with a practical example.\n",
      "\n",
      "🎯 Subject: cooking, Topic: seasoning\n",
      "   System: You are a helpful assistant that explains cooking concepts clearly.\n",
      "   Human:  Explain seasoning in simple terms with a practical example.\n",
      "\n",
      "🎯 Subject: gardening, Topic: composting\n",
      "   System: You are a helpful assistant that explains gardening concepts clearly.\n",
      "   Human:  Explain composting in simple terms with a practical example.\n",
      "\n",
      "🔗 The Chain Concept:\n",
      "   prompt | llm | output_parser\n",
      "   ↓\n",
      "   Input → Format Prompt → Send to LLM → Parse Response → Final Output\n",
      "\n",
      "🔧 Output Parser:\n",
      "   Converts LLM response objects into simple strings\n",
      "   Makes the output easy to work with in your code\n",
      "\n",
      "============================================================\n",
      "💡 Key Concepts Demonstrated:\n",
      "• Prompt templates with variables {like_this}\n",
      "• Message types: system (instructions) and human (user input)\n",
      "• The pipe operator | chains components together\n",
      "• Output parsers clean up the LLM response\n",
      "\n",
      "Next: Try running this with an actual LLM!\n"
     ]
    }
   ],
   "source": [
    "# Quick Demo: LangChain Prompt Templates (No API Key Required)\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "print(\"🔗 Understanding LangChain Building Blocks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that explains {subject} concepts clearly.\"),\n",
    "    (\"human\", \"Explain {topic} in simple terms with a practical example.\")\n",
    "])\n",
    "\n",
    "print(\"✅ Step 1: Prompt Template Created\")\n",
    "print(\"📝 This template has two variables: {subject} and {topic}\")\n",
    "print()\n",
    "\n",
    "# 2. Show how prompt formatting works\n",
    "subjects_and_topics = [\n",
    "    (\"programming\", \"variables\"),\n",
    "    (\"cooking\", \"seasoning\"),\n",
    "    (\"gardening\", \"composting\")\n",
    "]\n",
    "\n",
    "print(\"📋 See how the template gets filled with actual values:\")\n",
    "print()\n",
    "\n",
    "for subject, topic in subjects_and_topics:\n",
    "    # Format the prompt with actual values\n",
    "    formatted_messages = prompt.format_messages(subject=subject, topic=topic)\n",
    "    \n",
    "    print(f\"🎯 Subject: {subject}, Topic: {topic}\")\n",
    "    print(f\"   System: {formatted_messages[0].content}\")\n",
    "    print(f\"   Human:  {formatted_messages[1].content}\")\n",
    "    print()\n",
    "\n",
    "# 3. Show the chain concept\n",
    "print(\"🔗 The Chain Concept:\")\n",
    "print(\"   prompt | llm | output_parser\")\n",
    "print(\"   ↓\")\n",
    "print(\"   Input → Format Prompt → Send to LLM → Parse Response → Final Output\")\n",
    "print()\n",
    "\n",
    "# 4. Show what the output parser does\n",
    "output_parser = StrOutputParser()\n",
    "print(\"🔧 Output Parser:\")\n",
    "print(\"   Converts LLM response objects into simple strings\")\n",
    "print(\"   Makes the output easy to work with in your code\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"💡 Key Concepts Demonstrated:\")\n",
    "print(\"• Prompt templates with variables {like_this}\")\n",
    "print(\"• Message types: system (instructions) and human (user input)\")\n",
    "print(\"• The pipe operator | chains components together\") \n",
    "print(\"• Output parsers clean up the LLM response\")\n",
    "print(\"\\nNext: Try running this with an actual LLM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d2e97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Complete LangChain Flow (Simulated)\n",
      "==================================================\n",
      "🔗 Building the chain: prompt | mock_llm | output_parser\n",
      "\n",
      "📝 Question: Explain variables in programming\n",
      "   → Formatted prompt created\n",
      "   → LLM generated response\n",
      "   → Output parsed\n",
      "✅ Final Answer: Variables are like containers that store information in programming. For example, if you write 'name = \"Alice\"', you're storing the text 'Alice' in a container called 'name' that you can use later in your code.\n",
      "--------------------------------------------------\n",
      "📝 Question: Explain seasoning in cooking\n",
      "   → Formatted prompt created\n",
      "   → LLM generated response\n",
      "   → Output parsed\n",
      "✅ Final Answer: Seasoning is adding salt, herbs, and spices to enhance food flavor. For example, adding a pinch of salt and some black pepper to scrambled eggs makes them taste much better than plain eggs.\n",
      "--------------------------------------------------\n",
      "📝 Question: Explain composting in gardening\n",
      "   → Formatted prompt created\n",
      "   → LLM generated response\n",
      "   → Output parsed\n",
      "✅ Final Answer: Composting is recycling organic waste into nutrient-rich soil. For example, you can put fruit peels, coffee grounds, and fallen leaves in a bin, and over time they decompose into rich fertilizer for your garden.\n",
      "--------------------------------------------------\n",
      "\n",
      "💡 This is exactly what happens in a real LangChain application:\n",
      "1. Your input fills the prompt template variables\n",
      "2. The formatted prompt goes to the LLM (like GPT-3.5)\n",
      "3. The LLM generates a response\n",
      "4. The output parser cleans it up for your app\n",
      "\n",
      "🔑 To make this work with real AI, just add your OpenAI API key!\n"
     ]
    }
   ],
   "source": [
    "# Complete Example: What happens when you add an LLM to the chain\n",
    "print(\"🤖 Complete LangChain Flow (Simulated)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate what would happen with a real LLM\n",
    "class MockLLM:\n",
    "    \"\"\"A mock LLM that simulates responses for demonstration\"\"\"\n",
    "    \n",
    "    def invoke(self, messages):\n",
    "        # Get the human message (the actual question)\n",
    "        human_msg = messages[-1].content\n",
    "        \n",
    "        # Simple mock responses based on keywords\n",
    "        if \"variables\" in human_msg.lower():\n",
    "            return \"Variables are like containers that store information in programming. For example, if you write 'name = \\\"Alice\\\"', you're storing the text 'Alice' in a container called 'name' that you can use later in your code.\"\n",
    "        elif \"seasoning\" in human_msg.lower():\n",
    "            return \"Seasoning is adding salt, herbs, and spices to enhance food flavor. For example, adding a pinch of salt and some black pepper to scrambled eggs makes them taste much better than plain eggs.\"\n",
    "        elif \"composting\" in human_msg.lower():\n",
    "            return \"Composting is recycling organic waste into nutrient-rich soil. For example, you can put fruit peels, coffee grounds, and fallen leaves in a bin, and over time they decompose into rich fertilizer for your garden.\"\n",
    "        else:\n",
    "            return \"I'd be happy to explain that topic with a practical example!\"\n",
    "\n",
    "# Create the complete chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that explains {subject} concepts clearly.\"),\n",
    "    (\"human\", \"Explain {topic} in simple terms with a practical example.\")\n",
    "])\n",
    "\n",
    "mock_llm = MockLLM()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "print(\"🔗 Building the chain: prompt | mock_llm | output_parser\")\n",
    "print()\n",
    "\n",
    "# Test the complete flow\n",
    "test_cases = [\n",
    "    (\"programming\", \"variables\"),\n",
    "    (\"cooking\", \"seasoning\"), \n",
    "    (\"gardening\", \"composting\")\n",
    "]\n",
    "\n",
    "for subject, topic in test_cases:\n",
    "    print(f\"📝 Question: Explain {topic} in {subject}\")\n",
    "    \n",
    "    # Step 1: Format the prompt\n",
    "    formatted_messages = prompt.format_messages(subject=subject, topic=topic)\n",
    "    print(f\"   → Formatted prompt created\")\n",
    "    \n",
    "    # Step 2: Send to LLM\n",
    "    llm_response = mock_llm.invoke(formatted_messages)\n",
    "    print(f\"   → LLM generated response\")\n",
    "    \n",
    "    # Step 3: Parse output\n",
    "    final_output = output_parser.invoke(llm_response)\n",
    "    print(f\"   → Output parsed\")\n",
    "    \n",
    "    print(f\"✅ Final Answer: {final_output}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n💡 This is exactly what happens in a real LangChain application:\")\n",
    "print(\"1. Your input fills the prompt template variables\")\n",
    "print(\"2. The formatted prompt goes to the LLM (like GPT-3.5)\")  \n",
    "print(\"3. The LLM generates a response\")\n",
    "print(\"4. The output parser cleans it up for your app\")\n",
    "print(\"\\n🔑 To make this work with real AI, just add your OpenAI API key!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
